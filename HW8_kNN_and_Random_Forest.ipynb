{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FGrw6kiveUAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "1.   Implement (from scratch) and apply kNN on MNIST with k=1, 5, 10. Apply kNN on raw images, and 2, 7 dimensional eigenspaces (PCA, you can use a library for PCA, or your own code from HW 5), respectively. Show the accuracy scores for each run (you'd run the algorithm 9 times).\n",
        "\n",
        "2.   Use and run the Random Forest algorithm for MNIST classification (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Show the accuracy scores and the parameters you used.\n",
        "\n",
        "If it takes too much time, reduce the number of samples for training/testing (based on random selection)."
      ],
      "metadata": {
        "id": "XtZTSiDRAp2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import six.moves.cPickle as pickle\n",
        "import gzip\n",
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# load data\n",
        "def load_data(dataset):\n",
        "    if not os.path.isfile(dataset):\n",
        "        origin = (\n",
        "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "        )\n",
        "        print('Downloading data from %s' % origin)\n",
        "        urllib.request.urlretrieve(origin, dataset)\n",
        "\n",
        "    print('Loading data...')\n",
        "\n",
        "    # Load the dataset\n",
        "    with gzip.open(dataset, 'rb') as f:\n",
        "        try:\n",
        "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "        except:\n",
        "            train_set, valid_set, test_set = pickle.load(f)\n",
        "\n",
        "    print('... data has been loaded!')\n",
        "    return train_set, valid_set, test_set\n",
        "\n",
        "\n",
        "#Load the data into train, validation and test sets\n",
        "train_set, val_set, test_set = load_data('mnist.pkl.gz')\n",
        "\n",
        "#Separate each set into image vector (_x) and label (_y)\n",
        "train_x, train_y = train_set\n",
        "val_x, val_y = val_set\n",
        "test_x, test_y = test_set\n",
        "\n",
        "idx = random.sample(range(len(test_x)), 10000)\n",
        "train_x = [train_x[i] for i in idx]\n",
        "train_y = [train_y[i] for i in idx]\n",
        "\n",
        "idx = random.sample(range(len(test_x)), 1000)\n",
        "test_x = [test_x[i] for i in idx]\n",
        "test_y = [test_y[i] for i in idx]\n"
      ],
      "metadata": {
        "id": "J5cik59AByis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ac359c-5b5b-45f9-e542-d0e2902bb0b6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "... data has been loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implement kNN\n",
        "class kNN():\n",
        "  def __init__(self, k):\n",
        "    self.k = k\n",
        "\n",
        "  def euclidean(self, v1, v2):\n",
        "    return np.sqrt(np.sum((v1-v2)**2))\n",
        "\n",
        "  def fit(self, train_x, train_y):\n",
        "    self.train_x = train_x\n",
        "    self.train_y = train_y\n",
        "\n",
        "  def majority(self, neighbours):\n",
        "    values, counts = np.unique(neighbours, return_counts=True)\n",
        "    return values[np.argmax(counts)]\n",
        "\n",
        "  def get_neighbours(self, test_row):\n",
        "    distances = list()\n",
        "    for (train_row, train_class) in zip(self.train_x, self.train_y):\n",
        "      dist = self.euclidean(train_row, test_row)\n",
        "      distances.append((dist, train_class))\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "    neighbours = list()\n",
        "    for i in range(self.k):\n",
        "      neighbours.append(distances[i])\n",
        "    return neighbours\n",
        "\n",
        "  def predict(self, test_x):\n",
        "    preds = []\n",
        "    for test_row in test_x:\n",
        "      nearest_neighbours = self.get_neighbours(test_row)\n",
        "      majority = self.majority(nearest_neighbours)\n",
        "      preds.append(majority)\n",
        "    return np.array(preds)\n",
        "\n",
        "\n",
        "def accuracy(preds, label):\n",
        "  return 100 * (preds == label).mean()\n"
      ],
      "metadata": {
        "id": "QhRsIqskfuJb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# kNN experiment\n",
        "for j in [0,2,7]:\n",
        "  if j:\n",
        "    pca = PCA(n_components=j)\n",
        "    train = pca.fit_transform(train_x)\n",
        "    test = pca.fit_transform(test_x)\n",
        "  else :\n",
        "    train = train_x\n",
        "    test = test_x\n",
        "  for i in [1,5,10]:\n",
        "    knn = kNN(k=i)\n",
        "    knn.fit(train,train_y)\n",
        "    result = knn.predict(test)\n",
        "    if j:\n",
        "      print(f'(pca={j}, k={i}) accuracy: {accuracy(result, test_y):.3f} %')\n",
        "    else:\n",
        "      print(f'(raw, k={i}) accuracy: {accuracy(result, test_y):.3f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnd6rLcbmiiO",
        "outputId": "7e84665a-4410-4c3a-d81a-4b394b0e33f8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(raw, k=1) accuracy: 58.500 %\n",
            "(raw, k=5) accuracy: 95.200 %\n",
            "(raw, k=10) accuracy: 94.500 %\n",
            "(pca=2, k=1) accuracy: 6.300 %\n",
            "(pca=2, k=5) accuracy: 39.000 %\n",
            "(pca=2, k=10) accuracy: 40.400 %\n",
            "(pca=7, k=1) accuracy: 9.600 %\n",
            "(pca=7, k=5) accuracy: 38.200 %\n",
            "(pca=7, k=10) accuracy: 39.500 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# ramdom forest experiment\n",
        "n_estimators = [50, 100, 200]\n",
        "\n",
        "for i in range(3):\n",
        "  randf = RandomForestClassifier(n_estimators=n_estimators[i])\n",
        "  randf.fit(train_x, train_y)\n",
        "  result = randf.predict(test_x)\n",
        "  print(f'(n_estimators={n_estimators[i]}) accuracy: {accuracy(result, test_y):.3f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y8DIsz69SwU",
        "outputId": "04862a7a-52cc-48a3-c26a-3e0272b63194"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(n_estimators=50) accuracy: 94.600 %\n",
            "(n_estimators=100) accuracy: 94.900 %\n",
            "(n_estimators=200) accuracy: 95.000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report\n",
        "\n",
        "\n",
        "1.   What is the goal of classification algorithms?\n",
        "2.   Compare kNN and Random Forest in terms of bias/variance, statistical assumptions of data (which data do they work best with?), sample complexity (how much data do they need to perform acceptably well), computational complexity, and others if you want.\n",
        "3.   In your implementation, which algorithm had the highest accuracy? What are some of the factors that contributed to one being better that the other?\n",
        "4.   In terms of computation time, which algorithm was faster? Why?\n",
        "5.   Is it always better to choose the algorithm that has the best accuracy or the one that has the fastest computational time? Give yout opinion and some examples of applications.\n",
        "6.   Conclude with some thoughts and things you learned from this homework."
      ],
      "metadata": {
        "id": "TZPU5Iz9C2IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The goal of classification algorithms is to correctly find the class of the input so that data can be classified.\n",
        "\n",
        "2. In terms of bias/variance, kNN has low bias and high variance in low k values; vice versa, random forest has both low bias and low variance. In terms of statistical assumptions of data, kNN is good for data with low dimensionality, and random forest is good for data with high dimensionality. In terms of sample complexity, kNN needs lots of data samples, and random forest needs less data samples. In terms of computational complexity, kNN is simpler than random forest.\n",
        "\n",
        "3. In my implementation, random forest with n_estimators=200 showed the highest accuracy. In my case, a larger n_estimators value was the key to high accuracy.\n",
        "\n",
        "4. In terms of computation time, the random forest was faster than kNN. The most significant reason is the different methods of predicting. For kNN, it computes every distance when it starts predicting, but random forest computes most of the things before predicting when it makes the trees. There might be some optimization issues as well because kNN is implemented from scratch, but random forest is implemented from the sklearn library.\n",
        "\n",
        "5. In machine learning, there's no right answer that works for every situation. There are many dependencies that should be considered when choosing the algorithms.\n",
        "\n",
        "6. I've learned that the computational resources of the algorithm, like time and space, are really important for machine learning because it should be able to handle a large amount of data.\n"
      ],
      "metadata": {
        "id": "9ZSydGbU8yFJ"
      }
    }
  ]
}